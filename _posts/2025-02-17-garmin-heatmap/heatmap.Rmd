---
title: "Where in the world is Taylor Paddling?"
description: |
  A heatmap based on paddling workouts recorded on my Garmin watch
author:
  - name: Taylor
    url: {}
date: 2025-02-17
output:
  distill::distill_article:
    self_contained: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(leaflet)
library(sf)
library(jsonlite)
library(leaflet.extras)
library(kableExtra)
```

# Introduction
Over the past two years, I’ve logged dozens of paddling workouts with my Garmin watch. I wanted to visualize where I paddled the most in 2024—so I built this interactive heatmap! Here’s what I learned and how I made it:

# Acquiring the data
As any data person will tell you, acquiring the data is usually the most tedious part - and it was. In order to get the data I wanted, I had to log into my Garmin account, filter my activities to water sports, and navigate each individual recorded activity in order to download it as a GPX file. This was in order to preserve the location data associated with each activity, which is not available in the summary .csv available for download.

# Processing the data
Once I had an individual file for each activity (87 in total!), I had to aggregate it all into one file. I did this in a separate processing script, so that I could save one small, efficient file to this website and its associated GitHub repo. The code below will not run, it's just to showcase how I did this processing.

```{r eval=FALSE}
library(sf)
library(xml2)
library(dplyr)
library(purrr)
library(lubridate)
library(jsonlite)

# Define the folder containing GPX files
gpx_folder <- "data/2024 Garmin Data/"

# Get a list of all GPX files in the folder
gpx_files <- list.files(gpx_folder, pattern = "\\.gpx$", full.names = TRUE)
print(gpx_files)  # This should show a list of file paths

# Spot-check 
gpx_sample <- read_xml(gpx_files[1])  # Read first file to make sure it worked
print(gpx_sample)

# Now use a function to parse all of the GPX files:
extract_gpx_distance <- function(file) {
  gpx <- read_xml(file) %>% xml_ns_strip()  # Strip namespace
  
  coords <- gpx %>%
    xml_find_all("//trkpt") %>%
    map_df(~data.frame(
      Latitude = as.numeric(xml_attr(.x, "lat")),
      Longitude = as.numeric(xml_attr(.x, "lon")),
      Timestamp = xml_text(xml_find_first(.x, "time")),  # Extract timestamp
      File = basename(file)  # Keep track of source file
    ))
  
  if (nrow(coords) < 2) {
    return(NULL)  # Skip files with too few points
  }
  
  # Convert Timestamp to proper datetime format
  coords <- coords %>%
    mutate(
      Timestamp = ymd_hms(Timestamp),  # Convert to POSIXct
      Date = as.Date(Timestamp)  # Extract Date separately
    ) %>%
    arrange(Timestamp)  # Ensure chronological order
  
  # Compute distances between consecutive points
  coords <- coords %>%
    mutate(
      Distance_m = c(0, distHaversine(cbind(Longitude[-n()], Latitude[-n()]), 
                                      cbind(Longitude[-1], Latitude[-1]))),  # Compute distances
      Cumulative_Distance_km = cumsum(Distance_m) / 1000  # Convert meters to km
    )
  
  return(coords)
}

# Run the function on all files
all_gpx_data <- map_df(gpx_files, extract_gpx_distance)

# Save data for future use
write.csv(all_gpx_data, "2024_paddling_routes.csv", row.names = FALSE)

# Convert to a spatial object
paddling_sf <- st_as_sf(all_gpx_data, coords = c("Longitude", "Latitude"), crs = 4326)

# Save as GeoJSON
st_write(paddling_sf, "paddling_data.geojson", driver = "GeoJSON", append = FALSE)
```

And voila! Now I have a GeoJSON file available to make fun maps with!

But first, stats...

# General stats


```{r echo = FALSE, message = FALSE}
# Load paddling data
paddling_sf <- st_read("../../data/paddling_data.geojson", quiet = TRUE)  

# Ensure data is sorted by date & time
paddling_sf <- paddling_sf %>%
  arrange(File, Timestamp)

# Convert distance to miles (1 km = 0.621371 miles)
paddling_sf <- paddling_sf %>%
  mutate(Distance_miles = Cumulative_Distance_km * 0.621371)

# Summarize data
summary_table <- paddling_sf %>%
  group_by(File) %>%
  summarize(
    Date = unique(Date),  # Get unique date per session
    Start_Time = min(Timestamp, na.rm = TRUE),  # Earliest recorded time
    End_Time = max(Timestamp, na.rm = TRUE),    # Latest recorded time
    Session_Duration_mins = as.numeric(difftime(End_Time, Start_Time, units = "mins")), 
    Distance_miles = max(Distance_miles, na.rm = TRUE)  # Total distance for session
  ) %>%
  ungroup()

# Compute overall statistics
total_distance_miles <- sum(summary_table$Distance_miles, na.rm = TRUE)
total_time_hours <- sum(summary_table$Session_Duration_mins, na.rm = TRUE) / 60
num_sessions <- nrow(summary_table)
avg_distance_per_session <- mean(summary_table$Distance_miles, na.rm = TRUE)
avg_duration_per_session <- mean(summary_table$Session_Duration_mins, na.rm = TRUE)
earliest_start_time <- format(min(summary_table$Start_Time, na.rm = TRUE), "%H:%M:%S")
latest_end_time <- format(max(summary_table$End_Time, na.rm = TRUE), "%H:%M:%S")

# Create summary data frame
overall_summary <- data.frame(
  Metric = c("Total Distance Paddled (miles)", "Total Hours Paddled", "Number of Sessions",
             "Average Distance per Session (miles)", "Average Duration per Session (mins)",
             "Earliest Start Time of Day", "Latest End Time of Day"),
  Value = c(round(total_distance_miles, 2), round(total_time_hours, 2), num_sessions,
            round(avg_distance_per_session, 2), round(avg_duration_per_session, 2),
            earliest_start_time, latest_end_time)
)

# Display as a table in R Markdown
kable(overall_summary, col.names = c("Metric", "Value"))
```

On a side note, I don't believe that last metric - "Latest end time of day" - is correct. If it's 11pm, that's wayyy too late to be on the water. If it's 11am, that's far too early to be off the water, because most of our practices are in the evenings. I'm sure something just got recorded wrong there. 

# Trends & Insights

```{r echo = FALSE}
# Convert distance from meters to km, then to miles
paddling_sf <- paddling_sf %>%
  mutate(
    Distance_km = Distance_m / 1000,       # Convert meters → km
    Distance_miles = Distance_km / 1.60934, # Convert km → miles
    YearMonth = format(Timestamp, "%Y-%m") # Extract YYYY-MM for grouping
  )

# Summarize total miles per month
monthly_summary <- paddling_sf %>%
  group_by(YearMonth) %>%
  summarize(Total_Miles = sum(Distance_miles, na.rm = TRUE)) %>%
  ungroup()

# Convert YearMonth to a proper date format for ordering
monthly_summary <- monthly_summary %>%
  mutate(YearMonth = as.Date(paste0(YearMonth, "-01")))

# Plot bar chart
ggplot(monthly_summary, aes(x = YearMonth, y = Total_Miles)) +
  geom_col(fill = "steelblue", width = 25) +  # Bar chart with blue fill
  theme_minimal() +
  labs(
    title = "Total Paddling Distance per Month",
    x = "Month",
    y = "Total Distance (Miles)"
  ) +
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +  # Format X-axis
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Tilt labels for readability
```

Quite a normal distribution until we get to change season! And you can bet your ass I did absolutely zero paddling in October. A girl's gotta rest.

# Spatial heatmap!

```{r echo = FALSE}
# Load GeoJSON file
paddling_sf <- st_read("../../data/paddling_data.geojson", quiet = TRUE)

# Extract coordinates
heat_data <- st_coordinates(paddling_sf)
```

```{r echo = FALSE}
# Create heatmap
leaflet() %>%
  addProviderTiles("CartoDB.Positron") %>%
  addHeatmap(
    lng = heat_data[, 1], 
    lat = heat_data[, 2], 
    radius = 10, 
    blur = 15
  )
```


# Animation





