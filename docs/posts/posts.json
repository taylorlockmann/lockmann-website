[
  {
    "path": "posts/2024-01-27-sankey-diagram-job-search/",
    "title": "A Lesson in Patience: The classic job search Sankey diagram",
    "description": "Visualizing the job search process using a Sankey diagram",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2024-01-27",
    "categories": [],
    "contents": "\r\nBackground\r\nAfter graduating with my masters in 2022, I found myself on the classic post-grad job hunt. I knew without a doubt that I didn’t want to pursue a PhD, so the only option left was work (I was also very tired of being a broke college student). So with a newly minted resume and an all-too-optimistic view of the public sector and NGOs, I set out on that fateful quest.\r\nBesides the two years of grad school that had helped me prepare for the job search, the best foresight I had was to track my job applications and the steps taken throughout each. You know, for science.\r\nMany months later, after dozens of sleepless nights, hundreds of resume revisions, and one awful (short-lived) job experience, I finally landed at the company I work with now. And let me tell you, dear reader, that the pretty tales I had been told about professional networking and shaking hands at mixers was not at all what I encountered in the post-Covid job world. I never even got to use the trick of holding a wine glass and food plate in one hand in order to shake hands with the other we were taught.\r\nBut I did learn a lot about patience, persistence, professionalism, and myself (yeah, yeah, cliche I know). And once the job search was officially over, I also learned how to make a Sankey diagram, which I assume is what you are actually here for.\r\nWhile many data-related subReddits may be oversaturated with these visuals, I believe them to be one of the more reader-friendly ways of digesting a long and varied process quickly. Through my process I learned several ways to create a Sankey diagram, which I have chronicled below.\r\nWhat is a Sankey diagram?\r\nA Sankey diagram show the flow of something from one state to the next.\r\nInsert more background here\r\nMaybe include a sample\r\nData Collection\r\nMy goal in creating this Sankey diagram of my job search was to visualize each application’s life history. I tried to bin each phase of the application project into a simple to understand “Step”. Every time I heard back from a job app, I would add the response to the next “Step” column. After much tidying of my tracking Google Sheet, this is what the raw dataset looked like:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(networkD3)\r\nlibrary(here)\r\nlibrary(stringr)\r\nlibrary(gt)\r\n\r\n# Read in job data\r\njob_data1 <- read_csv(here(\"data\", \"Post-Bren Job Search - Sheet1.csv\"))\r\n\r\njob_data1 %>% \r\n  gt_preview()\r\n\r\n\r\n\r\n      Company\r\n      Name/Title\r\n      Type\r\n      Applied Date\r\n      Confirmation of Receipt?\r\n      Heard Back?\r\n      Applied\r\n      Step 1\r\n      Step 2\r\n      Step 3\r\n      Step 4\r\n      Step 5\r\n      Step 6\r\n      Step 7\r\n      Notes\r\n    1\r\nThe Nature Conservancy\r\nConservation Technology Associate\r\nJob\r\n1/10/2022\r\nYes\r\nYes\r\nApplied\r\nRejected\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA2\r\nNCEAS\r\nOcean Health Index Data Analyst Intern\r\nFellowship\r\n1/14/2022\r\nYes\r\nYes\r\nApplied\r\nRejected\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA3\r\nNOAA\r\nExplorer in Training\r\nFellowship\r\n2/5/2022\r\nYes\r\nNo\r\nApplied\r\nGhosted\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA4\r\nNASA\r\nDEVELOP\r\nFellowship\r\n2/25/2022\r\nYes\r\nYes\r\nApplied\r\nInterview\r\nOffered position\r\nTurned down\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA5\r\nAudubon Society & ESRI\r\nDangermond Fellow\r\nFellowship\r\n3/28/2022\r\nYes\r\nYes\r\nApplied\r\nRejected\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA6..43\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n44\r\nE Source\r\nData Analyst\r\nJob\r\n2/7/2023\r\nYes\r\nYes\r\nApplied\r\nPhone screening\r\nSecond Interview\r\nSkills Test\r\nPanel Interview\r\nOffered position\r\nAccepted\r\nCurrently working\r\nNA\r\n\r\nSankey Diagram using networkD3\r\nThe package I settled on using was networkD3. See more about this package here.\r\nOne notable feature of this package is that the data must be structured with only 2 columns: a “from” column and a “to” column. While I’m sure there are ways to do this more elegantly through code, I did it by manually re-working my data, which I use for this diagram. This was the result:\r\n\r\n\r\n# Read in job data\r\njob_data2 <- read_csv(here(\"data\", \"Post-Bren Job Search - Sheet2.csv\"))\r\n\r\n# Create a preview\r\njob_data2 %>% \r\n  gt_preview()\r\n\r\n\r\n\r\n      from\r\n      to\r\n    1\r\napplied\r\nrejected2\r\napplied\r\nrejected3\r\napplied\r\nghosted4\r\napplied\r\nfirst interview5\r\nfirst interview\r\noffered position6..75\r\n\r\n76\r\noffered position\r\naccepted\r\n\r\nSee the annotated code below:\r\n\r\n\r\n# Read in necessary packages\r\nlibrary(networkD3)\r\nlibrary(here)\r\nlibrary(stringr)\r\n\r\n# Count the number of occurrences of each combination of \"from\" and \"to\"\r\ncounts <- dplyr::count(job_data2, from, to) %>% \r\n  ungroup() %>% \r\n  rename(\"number\" = \"n\")\r\n\r\n# Turn our counts data into a simple data frame\r\ncounts_df <- data.frame(counts)\r\n\r\n# Create a data frame of nodes\r\nnodes <- data.frame(\r\n  name = c(as.character(counts_df$from),\r\n           as.character(counts_df$to)) %>% unique()\r\n)\r\n\r\n# With networkD3, connection must be provided using id, not using real name like in the links dataframe. So we need to reformat it.\r\ncounts_df$IDsource <- match(counts_df$from, nodes$name)-1\r\ncounts_df$IDtarget <- match(counts_df$to, nodes$name)-1\r\n\r\n# Turn the nodes into an official title\r\nnodes$name <- str_to_title(nodes$name)\r\n\r\n# Now create the diagram!\r\nq <- sankeyNetwork(Links = counts_df, \r\n                   Nodes = nodes,\r\n                   Source = \"IDsource\", \r\n                   Target = \"IDtarget\",\r\n                   Value = \"number\", \r\n                   NodeID = \"name\",\r\n                   sinksRight = FALSE,\r\n                   fontSize = 12)\r\n\r\nq\r\n\r\n\r\n\r\nAdding counts using JavaScript\r\nI wanted to add the counts from each node to the next, and the best way I could find to do this was using JavaScript. Now I know absolutely nothing about JavaScript, but I was able to lift and shift some code into this diagram thanks to this Stack Overflow post\r\n\r\n\r\n# Add node count to the diagram\r\nnode_count <-\r\n  'function(el, x){\r\n    d3.select(el).selectAll(\".node text\")\r\n      .text(d => d.name + \" (\" + d.value + \")\");\r\n  }'\r\n\r\n\r\nhtmlwidgets::onRender(x = q, jsCode = node_count)\r\n\r\n\r\n\r\nThe last item of this project would be to move the “Applied” label node to the other side of the starting node. This is also possible to do with JavaScript, but I have not been able to find a best way to do this at the time of publishing. If you have a neat trick to do this, please drop me a line!\r\nThanks for reading!\r\n-T\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-02-18T22:38:05-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-a/",
    "title": "Project Sample A",
    "description": "An exploration of cetacean species richness on the California coast.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\n# Read in the data all together\r\nspecies <- here(\"data\",\"ca_cetaceans\", \"ca_cetaceans\")\r\nspecies_files <- dir(species, full.names = TRUE, pattern = \"*.tif\")\r\n\r\n# Rasterize all these files together using raster::stack\r\ncetaceans_data <- raster::stack(species_files)\r\n\r\n# Write a function to determine if species are present in a cell, with a threshold of 0.6 meaning \"present\"\r\nis_present <- function(x, thresh = .6){\r\n  y <- ifelse(x >= thresh, 1, 0)\r\n  return(y)\r\n}\r\n\r\n# Apply the threshold function to our stack\r\nspecies_richness <- calc(cetaceans_data, fun = is_present)\r\n\r\n# Find out how many species are in each cell\r\nspecies_richness1 <- calc(species_richness, fun = sum, na.rm = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Now create the file to use for the CA coastline\r\n\r\nstates <- ne_download(scale = 110, type = \"states\", category = \"cultural\", returnclass = \"sf\")\r\n\r\nca_state <- states %>% \r\n  filter(name == \"California\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Crop the species raster to match the CA coastline\r\nspecies_raster_cropped <- crop(species_richness1, extent(ca_state))\r\n\r\n# Now turn this cropped raster into a dataframe\r\nspecies_richness_df <- raster::rasterToPoints(species_raster_cropped) %>%\r\n  as.data.frame() %>% \r\n  filter(layer != 0) # Filter out cells that have no data in them\r\n\r\n# Now we have the species richness dataframe with which to make our ggplot!\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Now plot everything together\r\n\r\nggplot()+\r\n  geom_raster(data = species_richness_df, aes(x = x, y = y, fill = layer))+\r\n  geom_sf(data = ca_state, fill = \"lemonchiffon2\")+\r\n  scale_fill_gradient(low = \"white\", high = \"mediumblue\", name = \"Likely number of cetacean species present\")+\r\n  theme_minimal()+\r\n  theme(panel.background = element_rect(fill = \"grey90\"))+\r\n  labs( x = \"Longitude\",\r\n        y = \"Latitude\",\r\n        title = \"Cetacean species richness on the California coast\")\r\n\r\n\r\n\r\n\r\nFigure 1: Map of species richness of cetaceans off the coast of California. Species richness is defined as the number of species likely to be present in a certain area. For a species to be considered “present,” a likelihood threshold of 0.6 was observed, meaning that there had to be a minimum 60% chance that a species would be present in order for it to be considered “present” in our analysis.\r\nNote: I purposely did not clip the extent of the map because I liked the aesthetics of seeing the whole state shape rather than just the extent covered by the species raster. To clip I would have inserted a line in my ggplot of coord_sf(xlim = c(-125, -115), ylim = c(32, 38)).\r\nSources:\r\nKaschner, K., Rius-Barile, J., Kesner-Reyes, K., Garilao, C., Kullander, S., Rees, T., & Froese, R. (2016). AquaMaps: Predicted range maps for aquatic species. www.aquamaps.org\r\nMade with Natural Earth. Free vector and raster map data @ naturalearthdata.com.\r\nEnd Project Sample A\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-project-sample-a/project-sample-a_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-04-20T14:39:36-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-b/",
    "title": "Project Sample B",
    "description": "Using binary logistic regression to test the feasibility of using plant height, canopy length, canopy width, and number of green leaves to determine species of palmetto.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\n\r\n\r\nhide\r\n\r\n# Read in data\r\n\r\npalmetto_data <- read_csv(here(\"data\", \"palmetto.csv\"), \r\n                          col_types = cols(.default = 'c')) %>% \r\n  mutate(height = as.numeric(height)) %>% \r\n  mutate(length = as.numeric(length)) %>% \r\n  mutate(width = as.numeric(width)) %>% \r\n  mutate(green_lvs = as.numeric(green_lvs))\r\n\r\n\r\n\r\nOverview\r\nThis report evaluates data on two palmetto species, and uses binary logistic regression to test the feasibility of using certain indicator variables (tree height, canopy length, canopy width, and number of green leaves) to determine the trees’ species. Species 1 represents Serenoa repens, while Species 2 represents Sabal etonia. For the purposes of this analysis, I will be simply referring to them as “Species 1” and “Species 2” respectively unless stated otherwise.\r\nData source: Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\r\nResults\r\n1. Exploratory Visualizations\r\nPrinciple Components Analysis (PCA)\r\n\r\n\r\nhide\r\n\r\n# Make PCA subset\r\n\r\npalmetto_pca <- palmetto_data %>% \r\n  select(height, length, width, green_lvs) %>% \r\n  drop_na() %>% \r\n  scale() %>% \r\n  prcomp()\r\n\r\n# Make another subset for labeling\r\npalmetto_complete <- palmetto_data %>% \r\n  drop_na(height, length, width, green_lvs)\r\n\r\n# Make PCA biplot\r\n\r\nautoplot(palmetto_pca,\r\n         data = palmetto_complete,\r\n         colour = \"species\",\r\n         size = 1,\r\n         alpha = 0.7,\r\n         loadings = TRUE,\r\n         loadings.label = TRUE)+\r\n  scale_color_manual(values = c(\"goldenrod\", \"forestgreen\"))+\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nFigure 1: PCA biplot of our palmetto data. Principle component 1 (PC1) accounts for ~69% of variance in our data, and principle component 2 (PC2) accounts for ~20% of the variance in our data.\r\nKey takeaways:\r\nFrom our biplot, we can see the number of green leaves on a tree and the length of canopy have almost no correlation, as their loadings are at approximately 90 degrees to each other.\r\nTree height, canopy width, and canopy length all appear to be closely (positively) correlated, as they are loaded closely together.\r\nAdditionally, it appears the two species begin to diverge in similarity along increasing PC1.\r\nTree Height vs. Canopy Length\r\n\r\n\r\nhide\r\n\r\n# Other dataviz\r\n\r\nggplot(data = palmetto_data, aes(x = length, y = height))+\r\n  geom_point(aes(color=species), alpha = 0.7)+\r\n  scale_color_manual(values = c(\"goldenrod\", \"forestgreen\"))+\r\n  theme_bw()+\r\n  labs(x = \"Canopy Length\",\r\n       y = \"Height\")\r\n\r\n\r\n\r\n\r\nFigure 2: Height of trees vs. canopy length.\r\nKey takeaways:\r\nWe can see that in both species, canopy length generally increases with increasing tree height.\r\nHowever, Species 2 seems to tend to have a lower height overall.\r\n2. Binary Logistic Regression\r\nVariables:\r\nTree height (cm)\r\nCanopy length (cm)\r\nCanopy width(cm)\r\nNumber of green leaves\r\nFor this evaluation, our “0” reference level is Species 1, or Serenoa repens. Since a binary logistic regression calculates the probability of the non-zero factor, this indicates that our output will evaluate the probability that an observation is Species 2, or Sabal etonia.\r\n\r\n\r\nhide\r\n\r\n# Make a subset with our species numbers as factors. \r\n\r\npalmetto_blr_data <- palmetto_complete %>% \r\n  mutate(species = as.factor(species))\r\n\r\n# Double check the factor levels, and we see that \"Species 1\" is our 0 reference level. So left-hand side of equation will be probability of SPECIES 2.\r\n\r\npalmetto_blr <- glm(species ~ height + length + width + green_lvs,\r\n                    data = palmetto_blr_data,\r\n                    family = \"binomial\")\r\n\r\ntidy_palmetto_blr <- broom::tidy(palmetto_blr)\r\n\r\nfinal_palmetto_blr <- tidy_palmetto_blr %>% \r\n  mutate(p.value=case_when(p.value <= 0.0001 ~ \"<0.0001\"))\r\n\r\nkbl(\r\n  final_palmetto_blr,\r\n  col.names = c(\"Predictor Variable\", \"Coefficient\", \"Std. Error\", \"Statistic\", \"p-value\"),\r\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\r\n   digits = 3,\r\n  caption = \"Table 1: Results of binary linear regression on palmetto data.\") %>% \r\n  kable_styling(bootstrap_options = \"striped\",\r\n                full_width = F)\r\n\r\n\r\n\r\nTable 1: Table 1: Results of binary linear regression on palmetto data.\r\n\r\n\r\nPredictor Variable\r\n\r\n\r\nCoefficient\r\n\r\n\r\nStd. Error\r\n\r\n\r\nStatistic\r\n\r\n\r\np-value\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n3.227\r\n\r\n\r\n0.142\r\n\r\n\r\n22.712\r\n\r\n\r\n<0.0001\r\n\r\n\r\nheight\r\n\r\n\r\n-0.029\r\n\r\n\r\n0.002\r\n\r\n\r\n-12.670\r\n\r\n\r\n<0.0001\r\n\r\n\r\nlength\r\n\r\n\r\n0.046\r\n\r\n\r\n0.002\r\n\r\n\r\n24.556\r\n\r\n\r\n<0.0001\r\n\r\n\r\nwidth\r\n\r\n\r\n0.039\r\n\r\n\r\n0.002\r\n\r\n\r\n18.782\r\n\r\n\r\n<0.0001\r\n\r\n\r\ngreen_lvs\r\n\r\n\r\n-1.908\r\n\r\n\r\n0.039\r\n\r\n\r\n-49.107\r\n\r\n\r\n<0.0001\r\n\r\n\r\n3. Success of Regression Model\r\n\r\n\r\nhide\r\n\r\n# What are the actual probabilities of being Species 2 for each of the existing observations in our palmetto_complete data frame?\r\n\r\n# Create the fitted blr - probability that an observation is Species 2.\r\n\r\nblr_fitted <- palmetto_blr %>% \r\n  broom::augment(type.predict = \"response\")\r\n\r\n# Wrangle this dataset into something usable for an output\r\n\r\nblr_correct <- blr_fitted %>% \r\n  mutate(predicted = case_when(.fitted >= 0.50 ~ 2,\r\n                               .fitted <= 0.50 ~ 1)) %>% \r\n  mutate(correct = case_when(predicted == species ~ \"Correct\",\r\n                             TRUE ~ \"Incorrect\")) %>% \r\n  mutate(species = case_when(species == 1 ~ \"Serenoa repens\",\r\n                             species == 2 ~ \"Sabal etonia\")) %>%\r\n  select(species, .fitted, predicted, correct) %>% \r\n  group_by(species, correct) %>% \r\n  summarize(number_correct = n()) %>% \r\n  pivot_wider(names_from = correct, values_from = number_correct) %>%\r\n  mutate(Percent_Correct = (Correct/(Incorrect+Correct)*100))\r\n\r\n# Now make it pretty!\r\n\r\nkbl(\r\n  blr_correct,\r\n  col.names = c(\"Species\", \"Correctly Classified\", \"Incorrectly Classified\", \"Percent Correctly Classified\"),\r\n  digits = 3,\r\n  align = c(\"l\", \"c\", \"c\", \"c\"),\r\n  caption =  \"Table 2: Successful prediction of species by binary linear regression on palmetto data.\") %>% \r\n  kable_styling(bootstrap_options = \"striped\",\r\n                full_width = F) \r\n\r\n\r\n\r\nTable 2: Table 2: Successful prediction of species by binary linear regression on palmetto data.\r\n\r\n\r\nSpecies\r\n\r\n\r\nCorrectly Classified\r\n\r\n\r\nIncorrectly Classified\r\n\r\n\r\nPercent Correctly Classified\r\n\r\n\r\nSabal etonia\r\n\r\n\r\n5701\r\n\r\n\r\n454\r\n\r\n\r\n92.624\r\n\r\n\r\nSerenoa repens\r\n\r\n\r\n5548\r\n\r\n\r\n564\r\n\r\n\r\n90.772\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-project-sample-b/project-sample-b_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-04-20T14:39:36-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-c/",
    "title": "Project Sample C",
    "description": "Using multicriteria analysis (MCA) to determine priority watersheds for conservation in southern Santa Barbara County.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\nUsing MCA to determine priority watersheds for conservation\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-20T14:39:37-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-d/",
    "title": "Project Sample D",
    "description": "Principle components analysis (PCA) of nutritional values of breakfast cereals.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\nIntroduction\r\nThis analysis examines the relationships between major nutritional values of breakfast cereals produced by major manufacturers General Mills Inc., Kellogg Co., Post Foods LLC, and the Quaker Oats Co. These manufacturers were chosen as they are the largest producers of breakfast cereals by sales. Nutritional data is provided by the USDA FoodData Central. The major nutritional values explored are kcal, protein, fat, carbs, sugar, and fiber content, in accordance with FDA guidance.\r\nSource: https://fdc.nal.usda.gov/index.html\r\nWrangling and PCA\r\n1. Wrangle data\r\n\r\n\r\n# Read in the data we're using, then clean it up a bit.\r\n\r\nusda_nutrients <- read.csv(here(\"data\", \"usda_nutrients.csv\")) %>% \r\n  clean_names() %>% \r\n  mutate(across(where(is.character), tolower))\r\n\r\n# Create a subset of only the data we will be using - in this case, breakfast cereals. We will be comparing the major nutrient values: kcal, protein, carbs, fat, sugar and fiber.\r\n\r\ncereals_nutrients <- usda_nutrients %>% \r\n  filter(food_group == \"breakfast cereals\") %>% # Select for breakfast cereals.\r\n  filter(mfg_name %in% c(\"kellogg, co.\", \"the quaker oats, co.\", \"general mills inc.\", \"post foods, llc\")) # Select specifically for the four brands we are evaluating. \r\n\r\n\r\n\r\n2. PCA\r\n\r\n\r\n# Now write the code for PCA with the dataset we created above.\r\n\r\ncereals_pca <- cereals_nutrients %>% # Create a pca dataset starting with the cereals subset we created above.\r\n  select(energy_kcal, ends_with(\"_g\")) %>% # Select the variables we will be performing PCA for\r\n  drop_na() %>% # Drop any NA values \r\n  scale() %>% # Scale the values, since they are not all measured in the same units\r\n  prcomp() # Principle components\r\n\r\ncereals_complete <- cereals_nutrients %>% \r\n  drop_na(energy_kcal, ends_with(\"_g\")) # Create a dataset to pull from, then drop observations with NA for our variables of interest.\r\n\r\nautoplot(cereals_pca,\r\n         data = cereals_complete, # Use this dataset to create aesthetics\r\n         colour = 'mfg_name',\r\n         loadings = TRUE, # Add loadings arrows\r\n         loadings.label = TRUE,\r\n         loadings.colour = \"black\",\r\n         loadings.label.colour = \"black\",\r\n         loadings.label.vjust = -0.5)+\r\n  theme_minimal()+\r\n  scale_x_continuous(expand = c(0.05,0.05))+ # Expand limits so we can read the arrow labels\r\n  scale_color_manual(labels = c(\"General Mills\", \"Kellogg\", \"Post Foods\", \"Quaker Oats\"), values = c(\"cornflowerblue\", \"coral\", \"forestgreen\", \"darkblue\"))+\r\n  labs(colour = \"Manufacturer\")\r\n\r\n\r\n\r\n\r\nFigure 1: Biplot depicting principle component analysis (PCA) of six different nutritional variables among breakfast cereals produced by four different manufacturers.\r\nSummary\r\nFrom the above Principle Component Analysis and biplot, we can draw the following takeaways:\r\nFiber and protein content appear most closely correlated, as these arrows are closest together, thus indicating that the variance explained by these variables are similar.\r\nSugar and fiber content have a negative correlation in breakfast cereals, as the angle between their arrows is much larger than 90 degrees and thus nearly opposite.\r\nProtein and fiber content have a slight negative loading value on Principle Component 1 (PC1), while fat, kcal, carb and sugar content all have a positive loading value on PC1.\r\nSugar content has the only positive loading value on Principle Component 2 (PC2), while all other variables have a negative loading value on PC2.\r\nHowever, upon adding the variances explained by PC1 and PC2, we see that this biplot only explains approximately 61.75% of variance among the data we are evaluating. This indicates that further analysis and consideration is needed to confidently evaluate correlation between variables in this data set.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-project-sample-d/project-sample-d_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-04-20T14:39:37-07:00",
    "input_file": {}
  }
]
