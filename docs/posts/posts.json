[
  {
    "path": "posts/2024-04-01-reading-analysis/",
    "title": "2022 Reading Report",
    "description": "A visualization exercise based on self-collected reading session data.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2024-04-02",
    "categories": [],
    "contents": "\r\nBackground\r\nAfter getting a library card at the end of 2021, I was so excited to tear through as many books as I could. My inner 7-year-old was happy as a clam as I checked out book after book and rediscovered my love of reading as an adult.\r\nWith the turn of the new year and my newly found interest in data and R thanks to grad school, I was on the hunt for a project that I could build from the ground-up and make my own. So armed with my newly minted library card, I started writing down my reading sessions.\r\nData\r\nThroughout all of 2022, I recorded the book I was reading, the author, and several other specifics of the book (more on that later). For each reading session, I recorded the date, start time, stop time, and number of pages read. The final product came out like this:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(here)\r\nlibrary(janitor)\r\nlibrary(gt)\r\n\r\n# Read in data\r\nraw_data <- read_csv(\"../../data/reading_tracking.csv\")\r\n\r\n# Get a preview of the data\r\nraw_data %>% \r\n  gt_preview()\r\n\r\n\r\n\r\n      Book Title\r\n      Author\r\n      Genre\r\n      Category\r\n      Source\r\n      Book Start\r\n      Book Finish\r\n      Date\r\n      Start\r\n      Stop\r\n      Pages Read\r\n      Notes\r\n    1\r\nWe Free the Stars\r\nHafsah Faizal\r\nFantasy\r\nYA\r\nLibrary book\r\n1/1/2022\r\n1/23/2022\r\n1/1/2022\r\n22:00:00\r\n23:30:00\r\n67\r\nNA2\r\nWe Free the Stars\r\nHafsah Faizal\r\nFantasy\r\nYA\r\nLibrary book\r\n1/1/2022\r\n1/23/2022\r\n1/2/2022\r\n23:30:00\r\n23:52:00\r\n7\r\nNA3\r\nWe Free the Stars\r\nHafsah Faizal\r\nFantasy\r\nYA\r\nLibrary book\r\n1/1/2022\r\n1/23/2022\r\n1/4/2022\r\n22:00:00\r\n22:15:00\r\n10\r\nNA4\r\nWe Free the Stars\r\nHafsah Faizal\r\nFantasy\r\nYA\r\nLibrary book\r\n1/1/2022\r\n1/23/2022\r\n1/5/2022\r\n22:00:00\r\n22:37:00\r\n30\r\nNA5\r\nWe Free the Stars\r\nHafsah Faizal\r\nFantasy\r\nYA\r\nLibrary book\r\n1/1/2022\r\n1/23/2022\r\n1/7/2022\r\n22:30:00\r\n23:30:00\r\n57\r\nNA6..75\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n76\r\nA Darker Shade of Magic\r\nV.E. Schwab\r\nFantasy\r\nAdult\r\nLibrary book\r\n12/13/2022\r\n12/31/2022\r\n12/31/2022\r\n10:20:00\r\n12:55:00\r\n152\r\nNA\r\n\r\nData Cleaning\r\nBefore we start making visualizations, I’d like to take a look at the data type of each column.\r\n\r\n\r\nraw_data %>% \r\n  glimpse()\r\n\r\nRows: 76\r\nColumns: 12\r\n$ `Book Title`  <chr> \"We Free the Stars\", \"We Free the Stars\", \"We …\r\n$ Author        <chr> \"Hafsah Faizal\", \"Hafsah Faizal\", \"Hafsah Faiz…\r\n$ Genre         <chr> \"Fantasy\", \"Fantasy\", \"Fantasy\", \"Fantasy\", \"F…\r\n$ Category      <chr> \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", \"YA\"…\r\n$ Source        <chr> \"Library book\", \"Library book\", \"Library book\"…\r\n$ `Book Start`  <chr> \"1/1/2022\", \"1/1/2022\", \"1/1/2022\", \"1/1/2022\"…\r\n$ `Book Finish` <chr> \"1/23/2022\", \"1/23/2022\", \"1/23/2022\", \"1/23/2…\r\n$ Date          <chr> \"1/1/2022\", \"1/2/2022\", \"1/4/2022\", \"1/5/2022\"…\r\n$ Start         <time> 22:00:00, 23:30:00, 22:00:00, 22:00:00, 22:30…\r\n$ Stop          <time> 23:30:00, 23:52:00, 22:15:00, 22:37:00, 23:30…\r\n$ `Pages Read`  <dbl> 67, 7, 10, 30, 57, 46, 56, 85, 31, 49, 42, 26,…\r\n$ Notes         <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\r\n\r\nFrom this first go, we can see that the column headers are not quite conducive to easy coding, so I will want to standardize that using tidyr::clean_names(). We can also see that the “date,” “book start,” and “book finish” columns are not stored as a date data type like I want them to be. So now I will use lubridate::mdy() to specify the data type of those columns.\r\nClean up the raw data and take a look at the first few rows and their data types again\r\n\r\n\r\nclean_data <- raw_data %>% \r\n  clean_names()# Puts all column headers into snake_case\r\n\r\nclean_data$book_start <- lubridate::mdy(clean_data$book_start) # Specifies that this column should be read as a date in M-D-Y format\r\nclean_data$book_finish <- lubridate::mdy(clean_data$book_finish)\r\nclean_data$date <- lubridate::mdy(clean_data$date)\r\n  \r\n# See the first few rows\r\nhead(clean_data)\r\n\r\n# A tibble: 6 × 12\r\n  book_title       author genre category source book_start book_finish\r\n  <chr>            <chr>  <chr> <chr>    <chr>  <date>     <date>     \r\n1 We Free the Sta… Hafsa… Fant… YA       Libra… 2022-01-01 2022-01-23 \r\n2 We Free the Sta… Hafsa… Fant… YA       Libra… 2022-01-01 2022-01-23 \r\n3 We Free the Sta… Hafsa… Fant… YA       Libra… 2022-01-01 2022-01-23 \r\n4 We Free the Sta… Hafsa… Fant… YA       Libra… 2022-01-01 2022-01-23 \r\n5 We Free the Sta… Hafsa… Fant… YA       Libra… 2022-01-01 2022-01-23 \r\n6 We Free the Sta… Hafsa… Fant… YA       Libra… 2022-01-01 2022-01-23 \r\n# ℹ 5 more variables: date <date>, start <time>, stop <time>,\r\n#   pages_read <dbl>, notes <chr>\r\n\r\n# See the column names, data types, and entries for each column\r\nglimpse(clean_data)\r\n\r\nRows: 76\r\nColumns: 12\r\n$ book_title  <chr> \"We Free the Stars\", \"We Free the Stars\", \"We Fr…\r\n$ author      <chr> \"Hafsah Faizal\", \"Hafsah Faizal\", \"Hafsah Faizal…\r\n$ genre       <chr> \"Fantasy\", \"Fantasy\", \"Fantasy\", \"Fantasy\", \"Fan…\r\n$ category    <chr> \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", \"YA\", …\r\n$ source      <chr> \"Library book\", \"Library book\", \"Library book\", …\r\n$ book_start  <date> 2022-01-01, 2022-01-01, 2022-01-01, 2022-01-01,…\r\n$ book_finish <date> 2022-01-23, 2022-01-23, 2022-01-23, 2022-01-23,…\r\n$ date        <date> 2022-01-01, 2022-01-02, 2022-01-04, 2022-01-05,…\r\n$ start       <time> 22:00:00, 23:30:00, 22:00:00, 22:00:00, 22:30:0…\r\n$ stop        <time> 23:30:00, 23:52:00, 22:15:00, 22:37:00, 23:30:0…\r\n$ pages_read  <dbl> 67, 7, 10, 30, 57, 46, 56, 85, 31, 49, 42, 26, 4…\r\n$ notes       <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\r\n\r\nBasic Visualizations\r\nSummary Visualization\r\nUsing base R\r\n\r\n\r\ntab1 <- table(clean_data$book_title)\r\ntab1\r\n\r\n\r\n                  A Darker Shade of Magic \r\n                                        6 \r\n                            American Gods \r\n                                        4 \r\n                      Braiding Sweetgrass \r\n                                        2 \r\n                          Come as you are \r\n                                        2 \r\n              Eating my Way Through Italy \r\n                                        4 \r\n                                    Fable \r\n                                        3 \r\n                                 Namesake \r\n                                        6 \r\nOn Looking: Eleven Walks with Expert Eyes \r\n                                        4 \r\n                          Sky in the Deep \r\n                                        2 \r\n                        Sorcery of Thorns \r\n                                        8 \r\n                      The Lost Apothecary \r\n                                        1 \r\n              The Once and Future Witches \r\n                                        9 \r\n                   The Shadows Between Us \r\n                                        7 \r\n                          Throne of Glass \r\n                                        4 \r\n                        We Free the Stars \r\n                                       13 \r\n                      Yoga for Every Body \r\n                                        1 \r\n\r\nbarplot(tab1)\r\n\r\n\r\n\r\nThis graph is terrible. In essence, it shows the number of reading sessions per book. But it doesn’t even list all of the book titles, and is very rudimentary. I’ll be using ggplot2 to jazz it up a bit.\r\nUsing ggplot2\r\n\r\n\r\n# First, create a summary table\r\nsummary <- clean_data %>% \r\n  group_by(book_title) %>% \r\n  summarize(\r\n    sessions_count = n()\r\n  )\r\n\r\n# Now, create a pretty plot\r\nggplot(data = summary, aes(book_title, sessions_count))+\r\n  geom_col(fill = \"#F4A261\")+ # Add my own color manually\r\n  labs(\r\n    x = \"Book Title\",\r\n    y = \"Number of Reading Sessions\"\r\n  )+\r\n  coord_flip()+ # I want the x & y axes to be flipped so we can clearly read the book titles\r\n  theme_bw()\r\n\r\n\r\n\r\nPie Chart: Book Genre\r\n“Book genre” was admittedly a bit of a subjective categorization by myself. I basically categorized each book based on the point of the book:\r\nGuidebook: books telling the reader about a place.\r\nSelf Help: books telling the reader how to improve their lives.\r\nNonfiction: those books that were nonfiction but did not fall into either of the first two categories.\r\nFantasy: I basically lumped all fiction into one fantasy category.\r\nSide note: Did you know ggplot2 can make pie charts?? I didn’t until I wanted to make this chart. See code below for how.\r\n\r\n\r\n# Group data by category and number of books\r\npie_summary <- clean_data %>% \r\n  group_by(genre) %>% \r\n  summarize(\r\n    n = n_distinct(book_title)\r\n  )\r\n# Side note: I wouldn't have to perform this grouping/summarizing step if were to use geom_bar(), but grom_col() requires this step be done beforehand.\r\n\r\n# Set colors\r\npiechart_colors <- c(Guidebook = \"#2A9D8F\", `Self Help` = \"#264653\", Nonfiction = \"#F4A261\", Fantasy = \"#E76F51\")\r\n\r\n# Make the plot\r\nggplot(pie_summary)+\r\n  geom_col(aes(x = 1, y = n, fill = genre), position = \"fill\")+\r\n  coord_polar(theta = \"y\")+\r\n  scale_fill_manual(values = piechart_colors)+\r\n  theme_bw()+\r\n  theme(axis.title = element_blank(),\r\n        axis.text = element_blank(),\r\n        axis.ticks = element_blank(),\r\n        panel.grid.major = element_blank(),\r\n        panel.grid.minor = element_blank(),\r\n        panel.border = element_blank())\r\n\r\n\r\n\r\nInteractive Pie Chart!\r\nI wanted to make a version of this pie chart that includes interactive elements, which we can do using plotly. I’ll keep it simple here, and perhaps do a deeper dive of plotly in a later post.\r\n\r\n\r\nlibrary(plotly)\r\n\r\nfig <- plot_ly(pie_summary, \r\n               labels = ~genre, \r\n               values = ~n, \r\n               type = 'pie',\r\n               textposition = 'inside',\r\n               textinfo = 'label+percent',\r\n               insidetextfont = list(color = '#FFFFFF'),\r\n               hoverinfo = 'text',\r\n               text = ~paste(n, ' books'),\r\n               marker = list(colors = piechart_colors,\r\n                             line = list(color = '#FFFFFF', width = 1)),\r\n               showlegend = FALSE)\r\n\r\nfig <- fig %>% layout(title = 'Books Read by Genre',\r\n                      xaxis = list(showgrid = FALSE, zeroline = FALSE,\r\n                                   showticklabels = FALSE),\r\n                      yaxis = list(showgrid = FALSE, zeroline = FALSE,\r\n                                   showticklabels = FALSE))\r\n\r\nfig\r\n\r\n\r\n\r\nStats Boxplot\r\nEvaluate the basic statistics of pages read per session according to genre.\r\n\r\n\r\nclean_data %>%\r\n  ggplot(aes(x = genre, y = pages_read, fill = genre)) +\r\n    geom_boxplot() +\r\n    scale_fill_manual(values = piechart_colors) +\r\n    theme_bw() +\r\n    theme(\r\n      legend.position=\"none\",\r\n      plot.title = element_text(size=11)\r\n    ) +\r\n    ggtitle(\"Basic reading session statistics according to genre\") +\r\n    xlab(\"\")\r\n\r\n\r\n\r\nOkay so - theoretically, a boxplot is designed to help us better understand basic statistics comparatively between several groups. It is made of the following components:\r\nA center line that represents the mean of the respective group.\r\nBoth top and bottom lines that represent the upper and lower quartile values, respectively.\r\nTwo whiskers that extend from the box to the upper and lower limits of the data.\r\nDots located directly above or below the whiskers that represent outliers of that group.\r\nHowever, based on the extremely varied number of pages read per reading session, I wouldn’t say that we can extract much useful information from this plot - especially considering the small sample size of every category besides Fantasy (sue me).\r\nTime Series Visualizations\r\nBasic\r\n\r\n\r\nclean_data %>% \r\n  ggplot(aes(x = date, y = pages_read))+\r\n  geom_point()+\r\n  theme_bw()\r\n\r\n\r\n\r\nThis visual is fairly simple; it shows the pages read per reading session across the entire year. We can see there’s a gap around October. At the time I was reading The Lost Apothecary by Sarah Penner (HIGHLY RECOMMEND) and decided not to track the book as I was enjoying reading it so much. That does provide for a level of inconsistency in the data, however. Maybe we can dig into this in a bit more complex of a time series below.\r\nFancy\r\n\r\n\r\n# We need to use the hms package to make sure R is reading the time codes correctly.\r\nlibrary(hms)\r\nlibrary(wesanderson) # I also just want to use Wes Anderson colors for funsies\r\n\r\n# Define the breaks I want to display on the y axis\r\nbreaks <- c(\"00:00:00\", \"03:00:00\", \"06:00:00\", \"09:00:00\", \"12:00:00\", \"15:00:00\", \"18:00:00\", \"21:00:00\", \"23:30:00\")\r\n\r\n# Define the colors I want to use\r\ntimeseries_colors <- c('#CC6677', '#88CCEE', '#882255', '#44AA99')\r\n\r\n# Graph code\r\nthumbnail <- clean_data %>% \r\n  ggplot(aes(x = date, y = start))+\r\n  geom_point(aes(size = pages_read, color = genre))+\r\n  scale_color_manual(values = timeseries_colors)+\r\n  scale_y_reverse(labels = function(x) as_hms(x),\r\n                  breaks = as_hms(breaks))+\r\n  labs(title = \"2022 Reading Sessions\",\r\n       x = \"Date\",\r\n       y = \"Start Time\",\r\n       color = \"Genre\",\r\n       size = \"Pages Read\")+\r\n  theme_bw()+\r\n  theme(panel.background = element_rect(fill = \"#f6f6f6\",\r\n                                        colour = \"#f6f6f6\",\r\n                                        size = 0.5,\r\n                                        linetype = \"solid\"),\r\n        plot.background = element_rect(fill = \"#f6f6f6\"),\r\n        legend.background = element_rect(fill = \"#f6f6f6\"),\r\n        legend.key = element_rect(fill = \"#f6f6f6\")\r\n  )\r\nthumbnail\r\n\r\n\r\n# Save for thumbnail\r\n# ggsave(\"reading-thumbnail.png\", plot = thumbnail, width = 6, height = 4, dpi = 300)\r\n\r\n\r\nThis graph shows us the time of day of the reading session, as well as the genre and pages read for each reading session, all across the whole year. Much more advanced!\r\nSession-Level Analysis\r\nNow let’s take a closer look at some information about the actual reading sessions.\r\n\r\n\r\n# Convert start time to hour format\r\nclean_data <- clean_data %>%\r\n  mutate(start_hour = hour(start))  # Extracting hour from start time\r\n\r\n# Scatter plot of pages read vs. start hour\r\nggplot(clean_data, aes(x = start_hour, y = pages_read)) +\r\n  geom_point(alpha = 0.5, color = \"#264653\") +  # Scatter plot with transparency\r\n  geom_smooth(method = \"lm\", color = \"#E76F51\", se = TRUE) +  # Linear trend line with confidence interval\r\n  labs(title = \"Effect of Start Time on Pages Read\",\r\n       x = \"Start Time (Hour of Day)\",\r\n       y = \"Pages Read per Session\") +\r\n  theme_bw()\r\n\r\n\r\n\r\nBy adding a trendline using geom_smooth(method = \"lm\"), we can see the general relationship between pages read and start time suggests that earlier start times tend to produce a greater number of pages read. This could likely be attributed to the fact that start times in the middle of the day are more often on weekends, so I have more time to read leisurely. And conversely, if I am reading later at night I don’t have as much time to read, so am more likely to read fewer pages.\r\nConclusion\r\nThis was a fun experience in both tracking my own reading sessions, and reflecting on the insights I gained from the data. I’m not sure I would collect this same data again, but would probably get more in-depth. One analysis I would have liked to perform would have been a sentiment analysis, but that would require me to have recorded a more robust “Notes” section. Maybe in the future!\r\nThanks for reading!\r\n-T\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-04-01-reading-analysis/images/reading-thumbnail.png",
    "last_modified": "2025-02-17T19:47:01-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-01-27-sankey-diagram-job-search/",
    "title": "A Lesson in Patience: The classic job search Sankey diagram",
    "description": "Visualizing the job search process using a Sankey diagram",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2024-01-27",
    "categories": [],
    "contents": "\r\nBackground\r\nAfter graduating with my masters in 2022, I found myself on the classic post-grad job hunt. I knew without a doubt that I didn’t want to pursue a PhD, so the only option left was work (I was also very tired of being a broke college student). So with a newly minted resume and an all-too-optimistic view of the public sector and NGOs, I set out on that fateful quest.\r\nBesides the two years of grad school that had helped me prepare for the job search, the best foresight I had was to track my job applications and the steps taken throughout each. You know, for science.\r\nMany months later, after dozens of sleepless nights, hundreds of resume revisions, and one awful (short-lived) job experience, I finally landed at the company I work with now. And let me tell you, dear reader, that the pretty tales I had been told about professional networking and shaking hands at mixers was not at all what I encountered in the post-Covid job world. I never even got to use the trick of holding a wine glass and food plate in one hand in order to shake hands with the other we were taught.\r\nBut I did learn a lot about patience, persistence, professionalism, and myself (yeah, yeah, cliche I know). And once the job search was officially over, I also learned how to make a Sankey diagram, which I assume is what you are actually here for.\r\nWhile many data-related subReddits may be oversaturated with these visuals, I believe them to be one of the more reader-friendly ways of digesting a long and varied process quickly. Through my process I learned several ways to create a Sankey diagram, which I have chronicled below.\r\nWhat is a Sankey diagram?\r\nA Sankey diagram show the flow of something from one state to the next.\r\nInsert more background here\r\nMaybe include a sample\r\nData Collection\r\nMy goal in creating this Sankey diagram of my job search was to visualize each application’s life history. I tried to bin each phase of the application project into a simple to understand “Step”. Every time I heard back from a job app, I would add the response to the next “Step” column. After much tidying of my tracking Google Sheet, this is what the raw dataset looked like:\r\n\r\n\r\nlibrary(tidyverse)\r\nlibrary(networkD3)\r\nlibrary(here)\r\nlibrary(stringr)\r\nlibrary(gt)\r\n\r\n# Read in job data\r\njob_data1 <- read_csv(here(\"data\", \"Post-Bren Job Search - Sheet1.csv\"))\r\n\r\njob_data1 %>% \r\n  gt_preview()\r\n\r\n\r\n\r\n      Company\r\n      Name/Title\r\n      Type\r\n      Applied Date\r\n      Confirmation of Receipt?\r\n      Heard Back?\r\n      Applied\r\n      Step 1\r\n      Step 2\r\n      Step 3\r\n      Step 4\r\n      Step 5\r\n      Step 6\r\n      Step 7\r\n      Notes\r\n    1\r\nThe Nature Conservancy\r\nConservation Technology Associate\r\nJob\r\n1/10/2022\r\nYes\r\nYes\r\nApplied\r\nRejected\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA2\r\nNCEAS\r\nOcean Health Index Data Analyst Intern\r\nFellowship\r\n1/14/2022\r\nYes\r\nYes\r\nApplied\r\nRejected\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA3\r\nNOAA\r\nExplorer in Training\r\nFellowship\r\n2/5/2022\r\nYes\r\nNo\r\nApplied\r\nGhosted\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA4\r\nNASA\r\nDEVELOP\r\nFellowship\r\n2/25/2022\r\nYes\r\nYes\r\nApplied\r\nInterview\r\nOffered position\r\nTurned down\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA5\r\nAudubon Society & ESRI\r\nDangermond Fellow\r\nFellowship\r\n3/28/2022\r\nYes\r\nYes\r\nApplied\r\nRejected\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA\r\nNA6..43\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n44\r\nE Source\r\nData Analyst\r\nJob\r\n2/7/2023\r\nYes\r\nYes\r\nApplied\r\nPhone screening\r\nSecond Interview\r\nSkills Test\r\nPanel Interview\r\nOffered position\r\nAccepted\r\nCurrently working\r\nNA\r\n\r\nSankey Diagram using networkD3\r\nThe package I settled on using was networkD3. See more about this package here.\r\nOne notable feature of this package is that the data must be structured with only 2 columns: a “from” column and a “to” column. While I’m sure there are ways to do this more elegantly through code, I did it by manually re-working my data, which I use for this diagram. This was the result:\r\n\r\n\r\n# Read in job data\r\njob_data2 <- read_csv(here(\"data\", \"Post-Bren Job Search - Sheet2.csv\"))\r\n\r\n# Create a preview\r\njob_data2 %>% \r\n  gt_preview()\r\n\r\n\r\n\r\n      from\r\n      to\r\n    1\r\napplied\r\nrejected2\r\napplied\r\nrejected3\r\napplied\r\nghosted4\r\napplied\r\nfirst interview5\r\nfirst interview\r\noffered position6..75\r\n\r\n76\r\noffered position\r\naccepted\r\n\r\nSee the annotated code below:\r\n\r\n\r\n# Read in necessary packages\r\nlibrary(networkD3)\r\nlibrary(here)\r\nlibrary(stringr)\r\n\r\n# Count the number of occurrences of each combination of \"from\" and \"to\"\r\ncounts <- dplyr::count(job_data2, from, to) %>% \r\n  ungroup() %>% \r\n  rename(\"number\" = \"n\")\r\n\r\n# Turn our counts data into a simple data frame\r\ncounts_df <- data.frame(counts)\r\n\r\n# Create a data frame of nodes\r\nnodes <- data.frame(\r\n  name = c(as.character(counts_df$from),\r\n           as.character(counts_df$to)) %>% unique()\r\n)\r\n\r\n# With networkD3, connection must be provided using id, not using real name like in the links dataframe. So we need to reformat it.\r\ncounts_df$IDsource <- match(counts_df$from, nodes$name)-1\r\ncounts_df$IDtarget <- match(counts_df$to, nodes$name)-1\r\n\r\n# Turn the nodes into an official title\r\nnodes$name <- str_to_title(nodes$name)\r\n\r\n# Now create the diagram!\r\nq <- sankeyNetwork(Links = counts_df, \r\n                   Nodes = nodes,\r\n                   Source = \"IDsource\", \r\n                   Target = \"IDtarget\",\r\n                   Value = \"number\", \r\n                   NodeID = \"name\",\r\n                   sinksRight = FALSE,\r\n                   fontSize = 12)\r\n\r\nq\r\n\r\n\r\n\r\nAdding counts using JavaScript\r\nI wanted to add the counts from each node to the next, and the best way I could find to do this was using JavaScript. Now I know absolutely nothing about JavaScript, but I was able to lift and shift some code into this diagram thanks to this Stack Overflow post\r\n\r\n\r\n# Add node count to the diagram\r\nnode_count <-\r\n  'function(el, x){\r\n    d3.select(el).selectAll(\".node text\")\r\n      .text(d => d.name + \" (\" + d.value + \")\");\r\n  }'\r\n\r\n\r\nhtmlwidgets::onRender(x = q, jsCode = node_count)\r\n\r\n\r\n\r\nThe last item of this project would be to move the “Applied” label node to the other side of the starting node. This is also possible to do with JavaScript, but I have not been able to find a best way to do this at the time of publishing. If you have a neat trick to do this, please drop me a line!\r\nThanks for reading!\r\n-T\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2024-02-19T08:44:01-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-a/",
    "title": "Project Sample A",
    "description": "An exploration of cetacean species richness on the California coast.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\n# Read in the data all together\r\nspecies <- here(\"data\",\"ca_cetaceans\", \"ca_cetaceans\")\r\nspecies_files <- dir(species, full.names = TRUE, pattern = \"*.tif\")\r\n\r\n# Rasterize all these files together using raster::stack\r\ncetaceans_data <- raster::stack(species_files)\r\n\r\n# Write a function to determine if species are present in a cell, with a threshold of 0.6 meaning \"present\"\r\nis_present <- function(x, thresh = .6){\r\n  y <- ifelse(x >= thresh, 1, 0)\r\n  return(y)\r\n}\r\n\r\n# Apply the threshold function to our stack\r\nspecies_richness <- calc(cetaceans_data, fun = is_present)\r\n\r\n# Find out how many species are in each cell\r\nspecies_richness1 <- calc(species_richness, fun = sum, na.rm = TRUE)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Now create the file to use for the CA coastline\r\n\r\nstates <- ne_download(scale = 110, type = \"states\", category = \"cultural\", returnclass = \"sf\")\r\n\r\nca_state <- states %>% \r\n  filter(name == \"California\")\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Crop the species raster to match the CA coastline\r\nspecies_raster_cropped <- crop(species_richness1, extent(ca_state))\r\n\r\n# Now turn this cropped raster into a dataframe\r\nspecies_richness_df <- raster::rasterToPoints(species_raster_cropped) %>%\r\n  as.data.frame() %>% \r\n  filter(layer != 0) # Filter out cells that have no data in them\r\n\r\n# Now we have the species richness dataframe with which to make our ggplot!\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n# Now plot everything together\r\n\r\nggplot()+\r\n  geom_raster(data = species_richness_df, aes(x = x, y = y, fill = layer))+\r\n  geom_sf(data = ca_state, fill = \"lemonchiffon2\")+\r\n  scale_fill_gradient(low = \"white\", high = \"mediumblue\", name = \"Likely number of cetacean species present\")+\r\n  theme_minimal()+\r\n  theme(panel.background = element_rect(fill = \"grey90\"))+\r\n  labs( x = \"Longitude\",\r\n        y = \"Latitude\",\r\n        title = \"Cetacean species richness on the California coast\")\r\n\r\n\r\n\r\n\r\nFigure 1: Map of species richness of cetaceans off the coast of California. Species richness is defined as the number of species likely to be present in a certain area. For a species to be considered “present,” a likelihood threshold of 0.6 was observed, meaning that there had to be a minimum 60% chance that a species would be present in order for it to be considered “present” in our analysis.\r\nNote: I purposely did not clip the extent of the map because I liked the aesthetics of seeing the whole state shape rather than just the extent covered by the species raster. To clip I would have inserted a line in my ggplot of coord_sf(xlim = c(-125, -115), ylim = c(32, 38)).\r\nSources:\r\nKaschner, K., Rius-Barile, J., Kesner-Reyes, K., Garilao, C., Kullander, S., Rees, T., & Froese, R. (2016). AquaMaps: Predicted range maps for aquatic species. www.aquamaps.org\r\nMade with Natural Earth. Free vector and raster map data @ naturalearthdata.com.\r\nEnd Project Sample A\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-project-sample-a/project-sample-a_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2022-04-20T14:39:36-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-b/",
    "title": "Project Sample B",
    "description": "Using binary logistic regression to test the feasibility of using plant height, canopy length, canopy width, and number of green leaves to determine species of palmetto.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\n\r\n\r\nhide\r\n\r\n# Read in data\r\n\r\npalmetto_data <- read_csv(here(\"data\", \"palmetto.csv\"), \r\n                          col_types = cols(.default = 'c')) %>% \r\n  mutate(height = as.numeric(height)) %>% \r\n  mutate(length = as.numeric(length)) %>% \r\n  mutate(width = as.numeric(width)) %>% \r\n  mutate(green_lvs = as.numeric(green_lvs))\r\n\r\n\r\n\r\nOverview\r\nThis report evaluates data on two palmetto species, and uses binary logistic regression to test the feasibility of using certain indicator variables (tree height, canopy length, canopy width, and number of green leaves) to determine the trees’ species. Species 1 represents Serenoa repens, while Species 2 represents Sabal etonia. For the purposes of this analysis, I will be simply referring to them as “Species 1” and “Species 2” respectively unless stated otherwise.\r\nData source: Abrahamson, W.G. 2019. Survival, growth and biomass estimates of two dominant palmetto species of south-central Florida from 1981 - 2017, ongoing at 5-year intervals ver 1. Environmental Data Initiative. https://doi.org/10.6073/pasta/f2f96ec76fbbd4b9db431c79a770c4d5\r\nResults\r\n1. Exploratory Visualizations\r\nPrinciple Components Analysis (PCA)\r\n\r\n\r\nhide\r\n\r\n# Make PCA subset\r\n\r\npalmetto_pca <- palmetto_data %>% \r\n  select(height, length, width, green_lvs) %>% \r\n  drop_na() %>% \r\n  scale() %>% \r\n  prcomp()\r\n\r\n# Make another subset for labeling\r\npalmetto_complete <- palmetto_data %>% \r\n  drop_na(height, length, width, green_lvs)\r\n\r\n# Make PCA biplot\r\n\r\nautoplot(palmetto_pca,\r\n         data = palmetto_complete,\r\n         colour = \"species\",\r\n         size = 1,\r\n         alpha = 0.7,\r\n         loadings = TRUE,\r\n         loadings.label = TRUE)+\r\n  scale_color_manual(values = c(\"goldenrod\", \"forestgreen\"))+\r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nFigure 1: PCA biplot of our palmetto data. Principle component 1 (PC1) accounts for ~69% of variance in our data, and principle component 2 (PC2) accounts for ~20% of the variance in our data.\r\nKey takeaways:\r\nFrom our biplot, we can see the number of green leaves on a tree and the length of canopy have almost no correlation, as their loadings are at approximately 90 degrees to each other.\r\nTree height, canopy width, and canopy length all appear to be closely (positively) correlated, as they are loaded closely together.\r\nAdditionally, it appears the two species begin to diverge in similarity along increasing PC1.\r\nTree Height vs. Canopy Length\r\n\r\n\r\nhide\r\n\r\n# Other dataviz\r\n\r\nggplot(data = palmetto_data, aes(x = length, y = height))+\r\n  geom_point(aes(color=species), alpha = 0.7)+\r\n  scale_color_manual(values = c(\"goldenrod\", \"forestgreen\"))+\r\n  theme_bw()+\r\n  labs(x = \"Canopy Length\",\r\n       y = \"Height\")\r\n\r\n\r\n\r\n\r\nFigure 2: Height of trees vs. canopy length.\r\nKey takeaways:\r\nWe can see that in both species, canopy length generally increases with increasing tree height.\r\nHowever, Species 2 seems to tend to have a lower height overall.\r\n2. Binary Logistic Regression\r\nVariables:\r\nTree height (cm)\r\nCanopy length (cm)\r\nCanopy width(cm)\r\nNumber of green leaves\r\nFor this evaluation, our “0” reference level is Species 1, or Serenoa repens. Since a binary logistic regression calculates the probability of the non-zero factor, this indicates that our output will evaluate the probability that an observation is Species 2, or Sabal etonia.\r\n\r\n\r\nhide\r\n\r\n# Make a subset with our species numbers as factors. \r\n\r\npalmetto_blr_data <- palmetto_complete %>% \r\n  mutate(species = as.factor(species))\r\n\r\n# Double check the factor levels, and we see that \"Species 1\" is our 0 reference level. So left-hand side of equation will be probability of SPECIES 2.\r\n\r\npalmetto_blr <- glm(species ~ height + length + width + green_lvs,\r\n                    data = palmetto_blr_data,\r\n                    family = \"binomial\")\r\n\r\ntidy_palmetto_blr <- broom::tidy(palmetto_blr)\r\n\r\nfinal_palmetto_blr <- tidy_palmetto_blr %>% \r\n  mutate(p.value=case_when(p.value <= 0.0001 ~ \"<0.0001\"))\r\n\r\nkbl(\r\n  final_palmetto_blr,\r\n  col.names = c(\"Predictor Variable\", \"Coefficient\", \"Std. Error\", \"Statistic\", \"p-value\"),\r\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\r\n   digits = 3,\r\n  caption = \"Table 1: Results of binary linear regression on palmetto data.\") %>% \r\n  kable_styling(bootstrap_options = \"striped\",\r\n                full_width = F)\r\n\r\n\r\n\r\nTable 1: Table 1: Results of binary linear regression on palmetto data.\r\n\r\n\r\nPredictor Variable\r\n\r\n\r\nCoefficient\r\n\r\n\r\nStd. Error\r\n\r\n\r\nStatistic\r\n\r\n\r\np-value\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n3.227\r\n\r\n\r\n0.142\r\n\r\n\r\n22.712\r\n\r\n\r\n<0.0001\r\n\r\n\r\nheight\r\n\r\n\r\n-0.029\r\n\r\n\r\n0.002\r\n\r\n\r\n-12.670\r\n\r\n\r\n<0.0001\r\n\r\n\r\nlength\r\n\r\n\r\n0.046\r\n\r\n\r\n0.002\r\n\r\n\r\n24.556\r\n\r\n\r\n<0.0001\r\n\r\n\r\nwidth\r\n\r\n\r\n0.039\r\n\r\n\r\n0.002\r\n\r\n\r\n18.782\r\n\r\n\r\n<0.0001\r\n\r\n\r\ngreen_lvs\r\n\r\n\r\n-1.908\r\n\r\n\r\n0.039\r\n\r\n\r\n-49.107\r\n\r\n\r\n<0.0001\r\n\r\n\r\n3. Success of Regression Model\r\n\r\n\r\nhide\r\n\r\n# What are the actual probabilities of being Species 2 for each of the existing observations in our palmetto_complete data frame?\r\n\r\n# Create the fitted blr - probability that an observation is Species 2.\r\n\r\nblr_fitted <- palmetto_blr %>% \r\n  broom::augment(type.predict = \"response\")\r\n\r\n# Wrangle this dataset into something usable for an output\r\n\r\nblr_correct <- blr_fitted %>% \r\n  mutate(predicted = case_when(.fitted >= 0.50 ~ 2,\r\n                               .fitted <= 0.50 ~ 1)) %>% \r\n  mutate(correct = case_when(predicted == species ~ \"Correct\",\r\n                             TRUE ~ \"Incorrect\")) %>% \r\n  mutate(species = case_when(species == 1 ~ \"Serenoa repens\",\r\n                             species == 2 ~ \"Sabal etonia\")) %>%\r\n  select(species, .fitted, predicted, correct) %>% \r\n  group_by(species, correct) %>% \r\n  summarize(number_correct = n()) %>% \r\n  pivot_wider(names_from = correct, values_from = number_correct) %>%\r\n  mutate(Percent_Correct = (Correct/(Incorrect+Correct)*100))\r\n\r\n# Now make it pretty!\r\n\r\nkbl(\r\n  blr_correct,\r\n  col.names = c(\"Species\", \"Correctly Classified\", \"Incorrectly Classified\", \"Percent Correctly Classified\"),\r\n  digits = 3,\r\n  align = c(\"l\", \"c\", \"c\", \"c\"),\r\n  caption =  \"Table 2: Successful prediction of species by binary linear regression on palmetto data.\") %>% \r\n  kable_styling(bootstrap_options = \"striped\",\r\n                full_width = F) \r\n\r\n\r\n\r\nTable 2: Table 2: Successful prediction of species by binary linear regression on palmetto data.\r\n\r\n\r\nSpecies\r\n\r\n\r\nCorrectly Classified\r\n\r\n\r\nIncorrectly Classified\r\n\r\n\r\nPercent Correctly Classified\r\n\r\n\r\nSabal etonia\r\n\r\n\r\n5701\r\n\r\n\r\n454\r\n\r\n\r\n92.624\r\n\r\n\r\nSerenoa repens\r\n\r\n\r\n5548\r\n\r\n\r\n564\r\n\r\n\r\n90.772\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-project-sample-b/project-sample-b_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-04-20T14:39:36-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-c/",
    "title": "Project Sample C",
    "description": "Using multicriteria analysis (MCA) to determine priority watersheds for conservation in southern Santa Barbara County.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\nUsing MCA to determine priority watersheds for conservation\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-04-20T14:39:37-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-15-project-sample-d/",
    "title": "Project Sample D",
    "description": "Principle components analysis (PCA) of nutritional values of breakfast cereals.",
    "author": [
      {
        "name": "Taylor",
        "url": {}
      }
    ],
    "date": "2021-03-15",
    "categories": [],
    "contents": "\r\nIntroduction\r\nThis analysis examines the relationships between major nutritional values of breakfast cereals produced by major manufacturers General Mills Inc., Kellogg Co., Post Foods LLC, and the Quaker Oats Co. These manufacturers were chosen as they are the largest producers of breakfast cereals by sales. Nutritional data is provided by the USDA FoodData Central. The major nutritional values explored are kcal, protein, fat, carbs, sugar, and fiber content, in accordance with FDA guidance.\r\nSource: https://fdc.nal.usda.gov/index.html\r\nWrangling and PCA\r\n1. Wrangle data\r\n\r\n\r\n# Read in the data we're using, then clean it up a bit.\r\n\r\nusda_nutrients <- read.csv(here(\"data\", \"usda_nutrients.csv\")) %>% \r\n  clean_names() %>% \r\n  mutate(across(where(is.character), tolower))\r\n\r\n# Create a subset of only the data we will be using - in this case, breakfast cereals. We will be comparing the major nutrient values: kcal, protein, carbs, fat, sugar and fiber.\r\n\r\ncereals_nutrients <- usda_nutrients %>% \r\n  filter(food_group == \"breakfast cereals\") %>% # Select for breakfast cereals.\r\n  filter(mfg_name %in% c(\"kellogg, co.\", \"the quaker oats, co.\", \"general mills inc.\", \"post foods, llc\")) # Select specifically for the four brands we are evaluating. \r\n\r\n\r\n\r\n2. PCA\r\n\r\n\r\n# Now write the code for PCA with the dataset we created above.\r\n\r\ncereals_pca <- cereals_nutrients %>% # Create a pca dataset starting with the cereals subset we created above.\r\n  select(energy_kcal, ends_with(\"_g\")) %>% # Select the variables we will be performing PCA for\r\n  drop_na() %>% # Drop any NA values \r\n  scale() %>% # Scale the values, since they are not all measured in the same units\r\n  prcomp() # Principle components\r\n\r\ncereals_complete <- cereals_nutrients %>% \r\n  drop_na(energy_kcal, ends_with(\"_g\")) # Create a dataset to pull from, then drop observations with NA for our variables of interest.\r\n\r\nautoplot(cereals_pca,\r\n         data = cereals_complete, # Use this dataset to create aesthetics\r\n         colour = 'mfg_name',\r\n         loadings = TRUE, # Add loadings arrows\r\n         loadings.label = TRUE,\r\n         loadings.colour = \"black\",\r\n         loadings.label.colour = \"black\",\r\n         loadings.label.vjust = -0.5)+\r\n  theme_minimal()+\r\n  scale_x_continuous(expand = c(0.05,0.05))+ # Expand limits so we can read the arrow labels\r\n  scale_color_manual(labels = c(\"General Mills\", \"Kellogg\", \"Post Foods\", \"Quaker Oats\"), values = c(\"cornflowerblue\", \"coral\", \"forestgreen\", \"darkblue\"))+\r\n  labs(colour = \"Manufacturer\")\r\n\r\n\r\n\r\n\r\nFigure 1: Biplot depicting principle component analysis (PCA) of six different nutritional variables among breakfast cereals produced by four different manufacturers.\r\nSummary\r\nFrom the above Principle Component Analysis and biplot, we can draw the following takeaways:\r\nFiber and protein content appear most closely correlated, as these arrows are closest together, thus indicating that the variance explained by these variables are similar.\r\nSugar and fiber content have a negative correlation in breakfast cereals, as the angle between their arrows is much larger than 90 degrees and thus nearly opposite.\r\nProtein and fiber content have a slight negative loading value on Principle Component 1 (PC1), while fat, kcal, carb and sugar content all have a positive loading value on PC1.\r\nSugar content has the only positive loading value on Principle Component 2 (PC2), while all other variables have a negative loading value on PC2.\r\nHowever, upon adding the variances explained by PC1 and PC2, we see that this biplot only explains approximately 61.75% of variance among the data we are evaluating. This indicates that further analysis and consideration is needed to confidently evaluate correlation between variables in this data set.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-15-project-sample-d/project-sample-d_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-04-20T14:39:37-07:00",
    "input_file": {}
  }
]
